{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IHWSzCaWkpCI",
        "_kuAD6LRH-1h",
        "y_es3ACJb6Hs",
        "PtCrU1JscxWC",
        "yajJfQmdc1vS"
      ],
      "mount_file_id": "10vCEvfhy2wtX1x3gZ420HJNIox4qh-k4",
      "authorship_tag": "ABX9TyNIVSOGQlmLtkwQ+UrzK2wl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckabelin/daai/blob/main/VTD_DAAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Ventum Team Day AI Tutorial\n",
        "\n",
        "In this tutorial you will learn\n",
        "- Include the most common python libs for ai and data analytics\n",
        "- Use several ai methods to chat and integrate data\n",
        "- Learn to code your ai-use-case based on open source and/or open ai"
      ],
      "metadata": {
        "id": "3-zDzVtZ7o8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usefull reads and links:\n",
        "* https://medium.com/@akriti.upadhyay/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n",
        "* https://github.com/ckabelin/daai\n",
        "* https://huggingface.co/settings/gated-repos\n",
        "* https://www.langchain.com/\n",
        "* https://openai.com/\n",
        "* https://github.com/huggingface\n",
        "* https://huggingface.co/docs/hub/datasets-downloading"
      ],
      "metadata": {
        "id": "ZB4F4NW_7jZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "zbGlkVLJHqzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Install os prerequisites"
      ],
      "metadata": {
        "id": "IHWSzCaWkpCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "id": "SFmngU-WKcLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get upgrade -y"
      ],
      "metadata": {
        "id": "Ic_VtQuhKb-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libarchive-dev python3-doc python3-pil.imagetk python-pil-doc libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
      ],
      "metadata": {
        "id": "grxgUWbyJX2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libomp-dev"
      ],
      "metadata": {
        "id": "hzkg9VoDSTr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get --fix-missing install"
      ],
      "metadata": {
        "id": "s45Pj-XsKraN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get --fix-broken install"
      ],
      "metadata": {
        "id": "5rdBRmNWKVPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "_CQWpuzBGUXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get clean"
      ],
      "metadata": {
        "id": "Yog5TMfNBxkN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Install Python Requirements"
      ],
      "metadata": {
        "id": "zzfEah0LbmMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm requirements.txt"
      ],
      "metadata": {
        "id": "qpyNCXFmO_Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -H \"Cache-Control: no-cache, no-store\" \"https://raw.githubusercontent.com/ckabelin/daai/refs/heads/main/requirements-slim.txt\" > requirements.txt"
      ],
      "metadata": {
        "id": "EbqhzIChLJ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet pip"
      ],
      "metadata": {
        "id": "FMcDV9DbNaK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet -r requirements.txt"
      ],
      "metadata": {
        "id": "CJf8Xn1oQ5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Library Imports"
      ],
      "metadata": {
        "id": "yguk2LPbmDeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Base Google Dependencies"
      ],
      "metadata": {
        "id": "6EzpZIzEk8-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b22OPnNLNned"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import base64\n",
        "import requests\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "import datasets\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define AI Framework"
      ],
      "metadata": {
        "id": "_kuAD6LRH-1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Define basic env variables"
      ],
      "metadata": {
        "id": "2rEv82XqCbax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OAI_TOKEN')"
      ],
      "metadata": {
        "id": "Wfl9pDBGqSq6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sRGTytxCjKaW"
      },
      "outputs": [],
      "source": [
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HF_HOME'] = \"/root/.cache/huggingface/hub/\" # \"~/.chache/.huggingface/\" # \"./drive/MyDrive/huggingface/cache/\"\n",
        "\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = os.environ['HF_HOME']\n",
        "os.environ['HF_DATASETS_CACHE'] = os.environ['HF_HOME']\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.environ['HF_HOME']"
      ],
      "metadata": {
        "id": "lRtDha1t0_Qf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.config.DOWNLOADED_DATASETS_PATH = Path(os.environ['HF_HOME'])\n",
        "datasets.config.HF_DATASETS_CACHE = os.environ['HF_HOME']"
      ],
      "metadata": {
        "id": "dcyONjESIeVz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Define Model and Parameters"
      ],
      "metadata": {
        "id": "Q8F91tWfxcLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_llm: str = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "QbRZAj6dxah2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_llm: str = \"meta-llama/Llama-3.2-1B-Instruct\""
      ],
      "metadata": {
        "id": "Irm9BX54Ssks"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_embeddings: str = \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "ToT_YryP6dF-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_embeddings: str = \"sentence-transformers/all-mpnet-base-v2\""
      ],
      "metadata": {
        "id": "rtShr-uN6a3u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens_limit: int = 20000\n",
        "temperature: float = 1e-10 # 0.1\n",
        "top_p: float = 0.9"
      ],
      "metadata": {
        "id": "Y8CkICNv6bQw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Define global ai variables"
      ],
      "metadata": {
        "id": "ktO8rMdQZNUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = None\n",
        "embeddings = None\n",
        "db = None\n",
        "retriever = None\n",
        "\n",
        "chat_history = None\n",
        "memory = None\n",
        "llm_chain = None"
      ],
      "metadata": {
        "id": "2fj8kJFUZQYG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Initialize AI"
      ],
      "metadata": {
        "id": "rvYJuQnVzdzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Common ai & langchain imports"
      ],
      "metadata": {
        "id": "liT7HDWf8OnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1 Pydantic allows for object-oriented semantic prompting into data structures (objects)"
      ],
      "metadata": {
        "id": "y_es3ACJb6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, validator"
      ],
      "metadata": {
        "id": "zHgq1j0NYYGr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Import langchain dependencies"
      ],
      "metadata": {
        "id": "ko2AW8eLcCBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_core.language_models.base import BaseLanguageModel\n",
        "from langchain_core.prompts.base import BasePromptTemplate\n",
        "\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import OnlinePDFLoader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser"
      ],
      "metadata": {
        "id": "hpoSJaZrYvfR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3 Hugging Face imports"
      ],
      "metadata": {
        "id": "AeLjSSxp8TJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
        "\n",
        "from huggingface_hub import login, whoami\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "from langchain_huggingface.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "RFoaUwVk8WYV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Spl_TfRlHf",
        "outputId": "ba7dcc58-7eca-4a5b-be68-525665437576"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.4 OpenAI Imports"
      ],
      "metadata": {
        "id": "PtCrU1JscxWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# from langchain.llms import OpenAI\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "zrxBoJXfcxz2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.5 Liquid AI Imports"
      ],
      "metadata": {
        "id": "yajJfQmdc1vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import liquidai"
      ],
      "metadata": {
        "id": "r_U2zFczc4zT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Initialize LLM and Embeddings (Hugging Face)"
      ],
      "metadata": {
        "id": "pLbfzed4868m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whoami(token=os.environ['HUGGINGFACEHUB_API_TOKEN'])"
      ],
      "metadata": {
        "id": "Fx6maY8cnDzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=os.environ['HUGGINGFACEHUB_API_TOKEN'])"
      ],
      "metadata": {
        "id": "co96lQCDmvlI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 LLM"
      ],
      "metadata": {
        "id": "t1Qfpm0WVUJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1.1 Online LLM"
      ],
      "metadata": {
        "id": "EK_KTLDcWH_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_huggingface_online():\n",
        "  global llm\n",
        "\n",
        "  llm_endpoint = HuggingFaceEndpoint(\n",
        "      repo_id=model_llm,\n",
        "      task=\"text-generation\",\n",
        "      # max_new_tokens=512,\n",
        "      do_sample=False,\n",
        "      repetition_penalty=1.03,\n",
        "      temperature=temperature\n",
        "  )\n",
        "\n",
        "  llm = ChatHuggingFace(llm=llm_endpoint)"
      ],
      "metadata": {
        "id": "IHQIqWg1WMNG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1.2 Offline LLM"
      ],
      "metadata": {
        "id": "MDvFKesRWMre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_huggingface_offline():\n",
        "  global llm\n",
        "  snapshot_download(repo_id=model_llm)\n",
        "  llm = HuggingFacePipeline.from_model_id(\n",
        "      model_id=model_llm,\n",
        "      task=\"text-generation\",\n",
        "      pipeline_kwargs={\n",
        "          \"token\": os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
        "          # \"max_new_tokens\": 100,\n",
        "          # \"top_k\": 50,\n",
        "          \"temperature\": temperature,\n",
        "      }\n",
        "  )\n",
        "  llm.pipeline.tokenizer.pad_token_id = llm.pipeline.tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "MiaAVnKXeWZa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.2 Embeddings"
      ],
      "metadata": {
        "id": "vET_wWijVNz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2.1 Online Embeddings"
      ],
      "metadata": {
        "id": "-0I-PPJjVYAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings_huggingface_online():\n",
        "  global embeddings\n",
        "  embeddings = HuggingFaceEndpointEmbeddings(\n",
        "      model=model_embeddings,\n",
        "      task=\"feature-extraction\"\n",
        "  )"
      ],
      "metadata": {
        "id": "TaMKqZrn5EKf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2.2 Offline Embeddings"
      ],
      "metadata": {
        "id": "4-8g6E95Vby3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings_huggingface_offline():\n",
        "  global embeddings\n",
        "  snapshot_download(repo_id=model_embeddings)\n",
        "  embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_embeddings\n",
        "  )"
      ],
      "metadata": {
        "id": "8Djyz7UBVl61"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Initialize LLM and Embeddings (Open AI)"
      ],
      "metadata": {
        "id": "Y8m2erfZpQ9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_openai():\n",
        "  global llm\n",
        "  llm = ChatOpenAI(\n",
        "      api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
        "      model_name = \"gpt-4o\",\n",
        "      temperature = temperature,\n",
        "      top_p = top_p\n",
        "  )"
      ],
      "metadata": {
        "id": "PB7gcYSKqwoq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings_openai():\n",
        "  global embeddings\n",
        "  embeddings = OpenAIEmbeddings(\n",
        "      api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
        "      model=\"text-embedding-3-large\"\n",
        "  )"
      ],
      "metadata": {
        "id": "b0w_u_Ny5HXX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Initialize LLM and Embedding Instances"
      ],
      "metadata": {
        "id": "GB9vp74Cbohk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.1 Initialize Huggingface offline"
      ],
      "metadata": {
        "id": "AMAjcHoYb7fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_huggingface_offline()\n",
        "embeddings_huggingface_offline()"
      ],
      "metadata": {
        "id": "lQ7C1Yevb0gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.2 Initialize Huggingface online"
      ],
      "metadata": {
        "id": "7rNXdEoMcAbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_huggingface_online()\n",
        "embeddings_huggingface_online()"
      ],
      "metadata": {
        "id": "FF-qkntgb0Rq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.3 Initialize OpenAI"
      ],
      "metadata": {
        "id": "zviqAdmVcC4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai()\n",
        "embeddings_openai()"
      ],
      "metadata": {
        "id": "OwF4h_0Jbz38"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Initialize Vector DB (in Memory via FAISS)"
      ],
      "metadata": {
        "id": "NUyf1BET9AGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "loader = None"
      ],
      "metadata": {
        "id": "jbK-HM3G-A4m"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Get Data"
      ],
      "metadata": {
        "id": "boOFCs7JbB-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 Get data from url\n",
        "e. g. https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689"
      ],
      "metadata": {
        "id": "f-kc5J6E_xbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://eur-lex.europa.eu/legal-content/DE/TXT/PDF/?uri=OJ:L_202401689\""
      ],
      "metadata": {
        "id": "J3Vgufz3-Y5T"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.bund.net/fileadmin/user_upload_bund/publikationen/nachhaltigkeit/nachhaltigkeit_gutes_leben_sdgs.pdf\""
      ],
      "metadata": {
        "id": "Lk_XhULFOI6z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.bundesregierung.de/resource/blob/975274/1873516/6c607bb5f16993ef18440d9e0dae55cb/2021-03-10-dns-2021-finale-langfassung-barrierefrei-data.pdf?download=1\""
      ],
      "metadata": {
        "id": "oJI9UlBHOtCS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = OnlinePDFLoader(url)"
      ],
      "metadata": {
        "id": "u4HjJ5CWMrcF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 Get data from files"
      ],
      "metadata": {
        "id": "xP4Vx8wAQQrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"./drive/MyDrive/sample_data/\""
      ],
      "metadata": {
        "id": "tSn4a7yDN2R9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = base_path + \"nachhaltigkeit\""
      ],
      "metadata": {
        "id": "v269X1fIQdKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = base_path + \"euaiact\""
      ],
      "metadata": {
        "id": "19U82Im5QaMO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(path)"
      ],
      "metadata": {
        "id": "TaRYpSzpQWVX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Prepare text content"
      ],
      "metadata": {
        "id": "yrryTZad_2jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 1000\n",
        "chunk_overlap = 30"
      ],
      "metadata": {
        "id": "zpmKwzBAo6h6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator=\"\\n\")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"Docs loaded: \" + str(len(docs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m29CAJ3p-fj2",
        "outputId": "81213fc6-5d26-4e90-9cf9-af56bf083f4b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs loaded: 792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Create embeddings and build vector db"
      ],
      "metadata": {
        "id": "F2pL8Rwc_6Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "rZO5EwGE9G3t"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "AfycpAHwSUK2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Initialize misc basics like question set"
      ],
      "metadata": {
        "id": "oYjdUTs99IQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []"
      ],
      "metadata": {
        "id": "ujaZv4fvz3q5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(question: str):\n",
        "  global questions\n",
        "  questions.append(question)"
      ],
      "metadata": {
        "id": "Tlugq3s_yHKo"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Define your questions"
      ],
      "metadata": {
        "id": "79KWJUT6ym73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []"
      ],
      "metadata": {
        "id": "qQ80tbmePEra"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add(\"Verfasse eine Zusammenfassung in 500 Worten oder weniger.\")"
      ],
      "metadata": {
        "id": "WB0IGoJ1zMdk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add(\"Fasse die Bedeutsamsten Punkte in maximal 20 Stichpunkten mit kurzer Erklärung zusammen.\")"
      ],
      "metadata": {
        "id": "zsyM6nqZOgyg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add(\"Fasse §66 in weniger als 100 Worten zusammen.\")"
      ],
      "metadata": {
        "id": "M9TJvTZ8PIAX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add(\"Erstelle einen ausführlichen Vorschlag in Stichpunkten, wie sich eine KI nach den Kriterien des EU AI Acts gründlich bewerten und einschätzen lässt.\")"
      ],
      "metadata": {
        "id": "r7FyI0phJcL0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Execute"
      ],
      "metadata": {
        "id": "oQnGZYA9zs1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 Initialize Chat History"
      ],
      "metadata": {
        "id": "WV-i4tpxqDrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = InMemoryChatMessageHistory()"
      ],
      "metadata": {
        "id": "WrY64fScllVO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Initialize Chat Memory"
      ],
      "metadata": {
        "id": "tuBnG3R8qH8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True,\n",
        "    output_key='answer',\n",
        "    chat_memory=chat_history\n",
        ")"
      ],
      "metadata": {
        "id": "jC3ZHbQFReru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 Define Priming Prompt"
      ],
      "metadata": {
        "id": "hzV3iHFXqNCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "priming: str = \"\"\"You are an top-tier lawyer specializing in digital laws and especially ai.\n",
        "You analyze and interpret legal text with the highest proficiency and explain them in simple and easy to understand terms.\n",
        "Always respond in german.\"\"\""
      ],
      "metadata": {
        "id": "koCb-gfxou_k"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 Define Messages (system and human)"
      ],
      "metadata": {
        "id": "1dJra_TIqQWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    priming + \" The context is:\\n{context}\"\n",
        ")\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"{question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "MMcRI_P-oaEb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5 Initialize Conversation Chain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQnDsn8bqWJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    verbose=False,\n",
        "    combine_docs_chain_kwargs={\n",
        "        \"prompt\": ChatPromptTemplate.from_messages([\n",
        "            system_message_prompt,\n",
        "            human_message_prompt,\n",
        "        ]),\n",
        "    },\n",
        "    return_source_documents=True,\n",
        "    max_tokens_limit=max_tokens_limit\n",
        ")"
      ],
      "metadata": {
        "id": "pqNnqvjASNMe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6 All-in-one to accomodate huggingface restrictions"
      ],
      "metadata": {
        "id": "uITYzltW5ZLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6.1 Define ask function with reinit for each ask (saves memory)"
      ],
      "metadata": {
        "id": "brpTpaG_ad7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(question: str) -> str:\n",
        "  global llm_chain\n",
        "  global chat_history\n",
        "  global memory\n",
        "\n",
        "  chat_history = InMemoryChatMessageHistory()\n",
        "\n",
        "  memory = ConversationBufferMemory(\n",
        "      memory_key='chat_history',\n",
        "      return_messages=True,\n",
        "      output_key='answer',\n",
        "      chat_memory=chat_history\n",
        "  )\n",
        "\n",
        "  llm_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=retriever,\n",
        "      memory=memory,\n",
        "      verbose=False,\n",
        "      combine_docs_chain_kwargs={\n",
        "          \"prompt\": ChatPromptTemplate.from_messages([\n",
        "              system_message_prompt,\n",
        "              human_message_prompt,\n",
        "          ]),\n",
        "      },\n",
        "      return_source_documents=True\n",
        "  )\n",
        "\n",
        "  response = llm_chain.invoke(question)\n",
        "  chat_history = response[\"chat_history\"]\n",
        "  last: AIMessage = chat_history[-1]\n",
        "  answer = last.content\n",
        "  return answer"
      ],
      "metadata": {
        "id": "dGMuOBFf5ZnS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6.2 Define ask function keeping session memory"
      ],
      "metadata": {
        "id": "ODbxU8QPapvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(question: str) -> str:\n",
        "  global llm_chain\n",
        "  global chat_history\n",
        "  global memory\n",
        "\n",
        "  response = llm_chain.invoke(question)\n",
        "  chat_history = response[\"chat_history\"]\n",
        "  last: AIMessage = chat_history[-1]\n",
        "  answer = last.content\n",
        "  return answer"
      ],
      "metadata": {
        "id": "dGWq3TYG6_BW"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.7 Shoot questions"
      ],
      "metadata": {
        "id": "TgL8n7gGqZx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for question in questions:\n",
        "  answer = ask(question)\n",
        "  print(\"Question: \" + question)\n",
        "  print(\"Answer: \")\n",
        "  print(answer)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "Y5vURYeOzSh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8 Interactive prompt"
      ],
      "metadata": {
        "id": "WnXua5T-eQQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "again: bool = True\n",
        "while(again):\n",
        "  result = input(\"Enter your Question: \")\n",
        "  value = str(result)\n",
        "  if value == \"exit\":\n",
        "    again = False\n",
        "  if again:\n",
        "    question: str = value\n",
        "    answer = ask(question)\n",
        "    print(\"Question: \" + question)\n",
        "    print(\"Answer: \")\n",
        "    print(answer)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "nwRWy_UifET-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Advanced Concepts"
      ],
      "metadata": {
        "id": "l62NZWTQeTLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Working with data structures"
      ],
      "metadata": {
        "id": "sGrzNKWOeiYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.1 Define Base classes"
      ],
      "metadata": {
        "id": "O_1q8-ylhzIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Deque, List, Optional, Tuple"
      ],
      "metadata": {
        "id": "-9lsuOCTi0bX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Artikel(BaseModel):\n",
        "  artikel: int = Field(description=\"Nummer des Artikels\")\n",
        "  headline: str = Field(description=\"Titel des Artikels\")\n",
        "  summary: str = Field(description=\"Zusammenfassung in weniger als 50 Worten\")\n",
        "\n",
        "class Abschnitt(BaseModel):\n",
        "  abschnitt: int = Field(description=\"Nummer des Abschnitts\")\n",
        "  headline: str = Field(description=\"Titel des Abschnitts\")\n",
        "  summary: str = Field(description=\"Zusammenfassung in weniger als 50 Worten\")\n",
        "  artikel: List[Artikel] = Field(description=\"Alle Artikel des Abschnitts\")\n",
        "\n",
        "class Kapitel(BaseModel):\n",
        "  kapitel: int = Field(description=\"Nummer des Kapitels\")\n",
        "  headline: str = Field(description=\"Titel des Kapitels\")\n",
        "  summary: str = Field(description=\"Zusammenfassung in weniger als 50 Worten\")\n",
        "  abschnitte: List[Abschnitt] = Field(description=\"Alle Abschnitte des Kapitels\")\n",
        "\n",
        "class AutoSummaryResult(BaseModel):\n",
        "  name: str = Field(description=\"Titel des Dokuments\")\n",
        "  author: str = Field(description=\"Author des Dokuments\")\n",
        "  chapters: List[Kapitel] = Field(description=\"Liste aller Kapitel\")\n"
      ],
      "metadata": {
        "id": "yGcX-Qn0ejS9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.2 Ask the AI"
      ],
      "metadata": {
        "id": "DRM59T9CiQr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the parser for the Data Model\n",
        "parser = PydanticOutputParser(pydantic_object=AutoSummaryResult)\n",
        "\n",
        "# Define the prompt template\n",
        "template_text = priming + \"\"\"\n",
        "\n",
        "    {format_instructions}\n",
        "\n",
        "    SUMMARY: {summaries}\n",
        "\n",
        "    QUESTION: {question}\"\"\"\n",
        "\n",
        "summary_query = \"Gebe ausführliche und korrekte Informationen zu dem Text\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template_text,\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Build the retrieval chain\n",
        "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "HkOV1RBEicUl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "as_result = \"\"\n",
        "try:\n",
        "  # ask the question and process the result\n",
        "  summary = qa.invoke({\"question\": summary_query})\n",
        "  res = summary[\"answer\"]\n",
        "  if res.startswith(\"```json\"):\n",
        "      res = res[7:-3]\n",
        "  # res = json.dumps(res, indent = 4)\n",
        "  as_result = str(res)\n",
        "  print(as_result)\n",
        "except Exception as ex:\n",
        "  error = str(ex)\n",
        "  print(error)"
      ],
      "metadata": {
        "id": "lh110qf9jTJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Working with data sets"
      ],
      "metadata": {
        "id": "CEzUEi95ei4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the dataset name and the column containing the content\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "page_content_column = \"context\"  # or any other column you're interested in\n",
        "\n",
        "# Create a loader instance\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
        "\n",
        "# Load the data\n",
        "data = loader.load()\n",
        "\n",
        "print(\"Datasets loaded: \" + str(len(data)))\n",
        "\n",
        "# Display the first 15 entries\n",
        "data[:2]"
      ],
      "metadata": {
        "id": "pCeasVjz9k4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}